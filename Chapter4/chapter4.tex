%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Quarter Chapter *********************************
%*******************************************************************************

\chapter{Implementación y programación distribuida}

\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi


%llever a la seccion de entrenamiento e implementación
es necesario generar las secuencias de sucesos de eventos, con estas secuencias se podrán identificar patrones en el comportamiento de los work items. Esto es importante debido que se esta abordando un problema relacionado con aprendizaje estructura, es decir que existen una dependencia entre cada nuevo suceso de evento con el anterior. Adicional, esta sería la estructura que soportan los modelos que son implementados para este trabajo.

Varias tareas se deben tener en cuenta para generar dichas secuencias de sucesos, primero se deben separar las secuencias que están asociadas a un error con las secuencias que no lo están, luego a partir de la característica de tiempo se ordenan los registros y se genera la secuencias con orden temporal. Para nuestro caso se estudio, la longitud de las secuencias pueden variar bastante, puedes existir secuencias muy pequeñas, pero también secuencias muy extensas, el maximo de observaciones es hasta de seis mil.
La sigueinte figura ilustra el proceso de generación de las secuencias de sucesos. Figura
%hasta acá

%********************************** %First Section  **************************************
\section{Entrenamiento, validación y predicción de los modelos} %Section - 4.1 
\label{section4.1}

Como se mencionó en el capitulo dos, por cada elemento de trabajo se tiene una secuencia de suscesos, estas secuencias de sucesos tienen una longitud variable, y son almacenados en un dataframe con dos columnas, la primera indica el id del elemento de trabajo, y la segunda corresonde a la secuencias de proceso. A dicional a esta estructura se debe tener encuenta que las secuencias de suscesos estas clasificadas, es decir se tiene un conjunto de secuencias correspondientes a los elementos de trabajo que fallaron en la herramienta WfM, y se tiene otro conjunto de secuencias que no fallaron. Teniendo esta distinción se puede iniciar con el proceso de entrenamiento. Es importance mencionar que estos satoso involucran un problema estructural, esto debido a que existe una dependencia entre observaciones, siempre la siguiente depende de la anterior.

Para cada modelo que se valido, se realizó el entrenamiento de cada conjunto de datos, haciendo uso de las crosvalidation con 5 folds, a partir de esto se realizó el calculo de los indicadores que permiten determinar cuales son los mejores parametros para el modelo.

Luego de esto, es posible realizar la evalución de la capacidad de predicción de los modelos, dado que los modelos han sido almacenados previamente, ahora con esto es posible calcular a partir de la longitud de las secuencias el poder de predicción de los modelos, ....

Agregar figura flujo del proceso.



%********************************** %Second Section  **************************************
\section{Implementación de los algoritmos } %Section - 4.2 
\label{section4.2}

LA implementación de los modelos HMM y demas consiste basicamente en definir aleatoriamente los parametros del modelo, que son las matrices A, B, pi, una vez se tienen esotos valores se empieza con el procesamiento, es decir el forwar y el back, este procesos, 

codigo relevante para la implementación y librerías relevantes, blablabla



%********************************** %Third Section  **************************************
\section{Programación distribuida} %Section - 4.3 
\label{section4.3}

Dado que el proceso secuencial es tan deficiente, se procede con el uso de tecnicas de programación distribuidas, como sparl, blblblb donde se usa el repartitions, y el reduce, los cuales eficiente menet reducen considerablemente el rpceoso, tamién se almacenas la info para poder ejecutar procesos de recovery, bablalblb
